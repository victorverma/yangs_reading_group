@misc{chen_neural_2020,
	title = {Neural {Spatio}-{Temporal} {Point} {Processes}},
	url = {https://arxiv.org/abs/2011.04583v3},
	abstract = {We propose a new class of parameterizations for spatio-temporal point processes which leverage Neural ODEs as a computational method and enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. Central to our approach is a combination of continuous-time neural networks with two novel neural architectures, i.e., Jump and Attentive Continuous-time Normalizing Flows. This approach allows us to learn complex distributions for both the spatial and temporal domain and to condition non-trivially on the observed event history. We validate our models on data sets from a wide variety of contexts such as seismology, epidemiology, urban mobility, and neuroscience.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
	month = nov,
	year = {2020},
}

@misc{jia_neural_2020,
	title = {Neural {Jump} {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1905.10403},
	doi = {10.48550/arXiv.1905.10403},
	abstract = {Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Jia, Junteng and Benson, Austin R.},
	month = jan,
	year = {2020},
	note = {arXiv:1905.10403 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dupont_augmented_2019,
	title = {Augmented {Neural} {ODEs}},
	url = {http://arxiv.org/abs/1904.01681},
	doi = {10.48550/arXiv.1904.01681},
	abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	month = oct,
	year = {2019},
	note = {arXiv:1904.01681 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{grathwohl_ffjord:_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	doi = {10.48550/arXiv.1810.01367},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv:1810.01367 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@misc{chen2019neural,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{Snyder_1991,
	doi = {10.1007/978-1-4612-3166-0},
	url = {https://doi.org/10.1007%2F978-1-4612-3166-0},
	year = 1991,
	publisher = {Springer New York},
	author = {Donald L. Snyder and Michael I. Miller},
	title = {Random Point Processes in Time and Space}
}

@inproceedings{Nan2016,
author = {Du, Nan and Dai, Hanjun and Trivedi, Rakshit and Upadhyay, Utkarsh and Gomez-Rodriguez, Manuel and Song, Le},
title = {Recurrent Marked Temporal Point Processes: Embedding Event History to Vector},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939875},
doi = {10.1145/2939672.2939875},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1555â€“1564},
numpages = {10},
keywords = {marked temporal point process, stochastic process, recurrent neural network},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@incollection{hochreiter_2009,
	title = {Gradient {Flow} in {Recurrent} {Nets}: {The} {Difficulty} of {Learning} {LongTerm} {Dependencies}},
	isbn = {978-0-470-54403-7},
	shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
	url = {https://ieeexplore.ieee.org/document/5264952},
	language = {en},
	urldate = {2023-11-17},
	booktitle = {A {Field} {Guide} to {Dynamical} {Recurrent} {Networks}},
	publisher = {IEEE},
	collaborator = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J},
	year = {2009},
	doi = {10.1109/9780470544037.ch14},
}
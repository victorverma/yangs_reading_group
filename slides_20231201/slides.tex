\documentclass{beamer}

\mode<presentation> {
\usetheme{AnnArbor}
}

\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

\setbeamertemplate{caption}[numbered]

\newtheorem{proposition}{Proposition}
\def\P{\mathbb P}
\def\ind{\perp\!\!\!\perp}
\DeclareMathOperator*{\argmax}{arg\,max}

\title[Temporal Point Processes]{Temporal Point Processes}

\author{Aniket Jivani \inst{1} and Victor Verma \inst{2}}
\institute[U-M]{
\inst{1} Department of Mechanical Engineering, University of Michigan \and %
\inst{2} Department of Statistics, University of Michigan
}
\date[12/1/23]{12/1/23}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}{Outline}
%    \tableofcontents
%\end{frame}
\section{Counting Processes}
\begin{frame}{Definition}
    
\end{frame}

% add more frames, redo sections etc. etc.


\section{Temporal Point Processes}
\begin{frame}{Poisson Processes}
    
\end{frame}

\begin{frame}{Hawkes Processes}
    
\end{frame}

\begin{frame}{Simulation Methods}
    
\end{frame}

\section{Applications}
\begin{frame}{Solar Flares}
    
\end{frame}

\begin{frame}{Other Examples}
    
\end{frame}

\section{Neural STPP}
\begin{frame}{STPP}
    \textcolor{red}{we can also avoid STPP and just focus on the neural temporal Hawkes processes to make presentation more focused } - see \cite{jia_neural_2020}

    \emph{Reference for STPP:} RTQ Chen et al. 2020\cite{chen_neural_2020}

    \emph{Key Idea of Neural STPP:} Use Continuous Normalizing Flows (CNFs) to Compute likelihoods for highly complex spatio-temporal distributions

    \emph{Key Idea of Neural SDE:} 

    \textcolor{red}{Provide better motivation behind use of NNs}
\end{frame}

\begin{frame}{Neural ODEs}
    Neural ODE: Continuous update for hidden states of neural networks by passing them through classical ODE solvers. Loss function updated through integration of adjoint state equation - \emph{constant memory regardless of network depth} i.e. storing intermediate activations is not necessary in forward pass.

    \begin{itemize}
        \item Model arbitrary irregularly sampled time series - single network with \emph{depth} arising from interval of integration - more function evaluations carried out by the adaptive ODE solver.

        \item Learn dynamics from data for single or multiple simulations (\emph{parametrized} NODE that enables UQ)

        \item Computationally tractable and capable of learning well-regularized flows based on lifting spaces and ideas from optimal transport
    \end{itemize}
\end{frame}

\begin{frame}{Extension}
    \begin{itemize}
        \item System of interest evolves continuously with time, but may also be interrupted by stochastic events!

        \item Model continuous flow and influence of the jump i.e. event conditional intensity by separate networks!

        \item Learnt model can approximate intensity function for a number of classical point processes such as Hawkes and STPPs. \cite{jia_neural_2020,chen_neural_2020}
    \end{itemize}
\end{frame}

% \begin{frame}{Example}
    
% \end{frame}

% \begin{frame}{Continuous Normalizing Flows}
%     Change of variables

%     Computational difficulty (show highly constrained architectures)

%     Speed up through CNF
% \end{frame}

% \begin{frame}{Example}
%     Gaussian to multi-modal distribution
% \end{frame}
\begin{frame}{Notation}
    \begin{enumerate}
        \item $\mathcal{H} = \{\tau_j\}$: sequence of discrete events
        \item $N(t) = \Sigma_{\tau_j \in \mathcal{H}} H(t - \tau_j)$ where $H(t) = \begin{cases}
               0 \quad t \leq 0\\
               1 \quad \text{ otherwise}\\
            \end{cases}$: counting process or counting function

        \item \textcolor{red}{check} $\lambda_t = p(\text{ event at time t } | \mathcal{H}_t)$
    \end{enumerate}
\end{frame}



\begin{frame}{Proposed framework 1}

\end{frame}

\begin{frame}{Proposed framework 2}

\end{frame}

\begin{frame}{Proposed framework 3}


\end{frame}

\begin{frame}{STPP Definition}



The conditional intensity function of a spatio-temporal point process i.e. the instantaneous probability of the $i$th event occuring at time $t$ and position $\boldsymbol{x}$ \emph{given $i-1$ events} is specified by:

\begin{equation*}
\lambda^{\ast}(t, \boldsymbol{x}) = \lambda(t,\boldsymbol{x}\mid\mathcal{H}_t)\triangleq\lim_{\Delta t\downarrow0,\Delta\boldsymbol{x}\downarrow0}\frac{\mathbb{P}\left(t_i\in[t,t+\Delta t],\boldsymbol{x}_i\in B(\boldsymbol{x},\Delta\boldsymbol{x})\mid\mathcal{H}_t\right)}{|B(\boldsymbol{x},\Delta\boldsymbol{x})|\Delta t}
\end{equation*}

\end{frame}

\begin{frame}{Modeling the log-likelihood}
If $\mathcal{H}=\{(t_i,\boldsymbol{x}_i)\}_{i=1}^n$ is the sequence of event times and their associated locations, 
then the log-likelihood of $\mathcal{H}$ in a time interval of $[0, T]$ is given by \textcolor{red}{show sketch of derivation}

\begin{equation*}
    \log p\left(\mathcal{H}\right)=\sum_{i\operatorname{-}1}^n\log\lambda^*(t_i,\boldsymbol{x}_i)-\int_0^T\int_{\mathbb{R}^d}\lambda^*(\tau,\boldsymbol{x})\mathrm{~}d\boldsymbol{x}d\tau 
\end{equation*}
\end{frame}

\begin{frame}{Typical Solution Techniques}
    
\end{frame}

\begin{frame}{Approximations}
\begin{enumerate}
    \item Decompose conditional intensity function
    $$\lambda^*(t,\boldsymbol{x})=\lambda^*(t)\not p^*(\boldsymbol{x}\mid t)$$
    \item Simplify log-likelihood:
    $$\log p(\mathcal{H})=\underbrace{\sum_{i=1}^n\log\lambda^*(t_i)-\int_0^T\lambda^*(\tau)}_{\text{temporal log-likelihood}}+\underbrace{\sum_{i=1}^n\log p^*(\boldsymbol{x}_{t_i}^{(i)}|t_i)}_{\text{spatial log-likelihood}}$$
    \item Time varying CNF or jump CNF
\end{enumerate}
    
\end{frame}

\begin{frame}{Results}
    Need comparison with a regular estimation procedure
\end{frame}

\begin{frame}{References}
    \nocite{*}
    \printbibliography
\end{frame}

\end{document}
\documentclass{beamer}

\mode<presentation> {
\usetheme{AnnArbor}
}

\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

\AtEveryBibitem{
    \clearfield{doi}
    \clearfield{isbn}
    \clearfield{issn}
    \clearlist{language}
    \clearfield{note}
    \clearfield{url}
    \clearfield{urlyear}
}

\setbeamertemplate{caption}[numbered]

\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}[section]
\def\C{\mathbb C}
\def\I{\mathbb I}
\def\P{\mathbb P}
\def\R{\mathbb R}
\def\RV{\text{RV}}
\def\Z{{\mathbb Z}}
\def\FPR{\text{FPR}}
\def\TPR{\text{TPR}}
\def\sign{{\rm sign}}
\def\ind{\perp\!\!\!\perp}
\newcommand{\AROptPred}[3]{\hat{Y}_{#1 + #2}(#3)}
\newcommand{\approxAROptPred}[3]{\hat{Y}_{#1 + #2}(\hat{#3})}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title[Optimal Extreme Event Prediction \ldots]{Optimal Extreme Event Prediction in \\ Heavy-Tailed Time Series}

\author{Victor Verma, Yang Chen, Stilian Stoev}
\institute[]
{
Department of Statistics \\
University of Michigan
}
\date[1/19/24]{1/19/24}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
   \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Solar Flares}
    \begin{itemize}
        \item Solar flares: sudden, massive eruptions of electromagnetic radiation from the Sun's atmosphere
        \item Adverse effects of solar flares: 
        \begin{itemize}
            \item Radio blackouts
            \item Coronal mass ejection (CME) $\rightarrow$ electromagnetic pulse $\rightarrow$ electrical blackouts
            \item Solar energetic particle event (SEP) $\rightarrow$ irradiation of astronauts
        \end{itemize}
        \item Some notable incidents:
        \begin{itemize}
            \item 1989: Quebec's electrical grid was shut down for several hours
            \item 2022: Dozens of Starlink satellites were destroyed
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{The X-Ray Flux}
    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{flare_flux_example.png}
        \caption{The X-ray flux around the time of a strong flare.}
        \label{fig:flare_flux_example}
    \end{figure}
\end{frame}

\begin{frame}{The X-Ray Flux}
    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{flux_plot.png}
        \caption{The X-ray flux time series.}
        \label{fig:flux_plot}
    \end{figure}
\end{frame}

\begin{frame}{Flare Strength}
    Flare strength is determined by the peak (soft) X-ray flux.
    \begin{table}
        \centering
        \begin{tabular}{|c|c|}
            \hline
            Flare Class & X-Ray Flux Threshold ($\text{W} / \text{m}^2$) \\
            \hline
            A & $10^{-8}$ \\
            B & $10^{-7}$ \\
            C & $10^{-6}$ \\
            M & $10^{-5}$ \\
            X & $10^{-4}$ \\
            \hline
        \end{tabular}
        \caption{X-ray flux thresholds for the different flare classes}
        \label{tab:flare_classes}
    \end{table}

    \textbf{
        Goal:
        \begin{center}
            Forecast flares by predicting whether the flux will exceed a threshold
        \end{center}
    }
\end{frame}

\begin{frame}{Existing Work}
    Example: Deep Flare Net \cite{nishizuka2018deep, nishizuka2021oper}
    \begin{itemize}
        \item Predicts whether strong flare will occur in next 24 hours
        \item For M+ class flares, non-operational TSS = 0.8, operational = 0.24
    \end{itemize}    
    The flare forecasting problem has not been solved:
    \begin{itemize}
        \item Forecasting methods were compared in operational setting in \cite{leka2019acomII, leka2019acomIII}
        \item Goal: predict if C+ class or M+ class flare will occur in next 24 hours
        \item For both, no method attained a TSS over 0.5.
        \item ML-based methods tended to perform worse
    \end{itemize}
\end{frame}

\section{Foundations of Optimal Extreme Event Prediction}

\begin{frame}{The Statistical Problem}
    Let $Y$ be a random variable with CDF $F_Y$ and quantile function $F_Y^{-1}$. Let $X$ be a random vector in $\R^d$.

    \medskip
    
    \textbf{
    The flare forecasting problem is an instance of this general problem:
    \begin{center}
        Predict whether $Y > F_Y^{-1}(p)$ using $g(X)$ for some suitable $g$.
    \end{center}
    }
    Example:
    \begin{itemize}
        \item $Y$: flux at time $t + h$
        \item $X$: vector of fluxes at times $t, \ldots, t - \ell$
    \end{itemize}

    \medskip
    
    Assume that $F_Y$ is continuous at $F_Y^{-1}(p)$. Then $\P(Y > F_Y^{-1}(p)) = 1 - p$.
\end{frame}

\begin{frame}{Calibration of Predictors}
    Given $g$, we predict that $Y > F_Y^{-1}(p)$ when $g(X)$ lies in some set $S$.

    Indicators can be used to represent both the predictand and a predictor:
    \[
    \I(Y > F_Y^{-1}(p)), \ \I(g(X) \in S)
    \]

    Predictors should be calibrated:
    \begin{definition}
        $\I(g(X) \in S)$ is calibrated if $\P(g(X) \in S) = 1 - p = \P(Y > F_Y^{-1}(p))$.
    \end{definition}
    If $F_{g(X)}$ is continuous at $F_{g(X)}^{-1}(p)$, then $(F_{g(X)}^{-1}(p), \infty)$ is an obvious choice for $S$, as
    \[
    \I(g(X) \in S) = \I(g(X) > F_{g(X)}^{-1}(p)) = 1 - p.
    \]
\end{frame}

\begin{frame}{Optimality of Predictors}
    \begin{definition}
        $\I(g(X) \in S)$ is an optimal predictor of $\I(Y > F_Y^{-1}(p))$ if
        \begin{itemize}
            \item it is calibrated
            \item for any other calibrated predictor $\I(k(X) \in T)$,
            \begin{equation}\label{eq:optimality_cond}
                \P(Y > F_Y^{-1}(p) \mid g(X) \in S) \ge \P(Y > F_Y^{-1}(p) \mid k(X) \in T)
            \end{equation}
        \end{itemize}
    \end{definition}
    We call probabilities like those in \eqref{eq:optimality_cond} precisions and denote them by $\lambda_p$. The optimal precision is then $\P(Y > F_Y^{-1}(p) \mid g(X) \in S)$.
\end{frame}

\begin{frame}{Optimal Predictors in the General Case}
    \begin{itemize}
        \item Let $X$ and $Y$ have a joint density $f$ with respect to the Lebesgue measure on $\R^d \times \mathbb{R}$.
        \item Let $f_0$ be the conditional density of $X$ given that $Y \le F_Y^{-1}(p)$
        \item Let $f_1$ be the conditional density of $X$ given that $Y > F_Y^{-1}(p)$
    \end{itemize}
    Then
    \[
    f_0(x) = \frac{1}{p}\int_{-\infty}^{F_Y^{-1}(p)} f(x, y)\,dy, \
    f_1(x) = \frac{1}{1 - p}\int_{F_Y^{-1}(p)}^{\infty} f(x, y)\,dy
    \]
    \begin{theorem}
        Let $r(x) = f_1(x) / f_0(x)$. Suppose that $F_{r(X)}$ is continuous at $F_{r(X)}^{-1}(p)$. Then $\I(r(X) > F_{r(X)}^{-1}(p))$ is an optimal predictor of $\I(Y > F_Y^{-1}(p))$.
    \end{theorem}
\end{frame}

\begin{frame}{Optimal Predictors in Special Cases}
    \begin{theorem}
        Let $X \in \R^d$ and $\epsilon \in \R$ be independent and let $\sigma$ be a function that is positive on the range of $X$. Suppose that 
        \begin{equation}\label{eq:add_err_mod}
            Y = \mu(X) + \sigma(X)\epsilon.
        \end{equation}
        Then an optimal predictor of $\I(Y > F_Y^{-1}(p))$ is $\I(g(X) \ge F_{g(X)}^{-1}(p))$, where
        \[
        g(X) = \frac{\mu(X) - F_Y^{-1}(p)}{\sigma(X)},
        \]
        assuming that $F_{g(X)}$ is continuous at $F_{g(X)}^{-1}(p)$.
    \end{theorem}
\end{frame}

\begin{frame}{Optimal Predictors in Special Cases}
    \begin{corollary}
        When $\sigma(X)$ in \eqref{eq:add_err_mod} is constant with respect to $X$, and $F_{\mu(X)}$ is continuous at $F_{\mu(X)}^{-1}(p)$, then an optimal predictor of $\I(Y > F_Y^{-1}(p))$ is
        \[
        \I(\mu(X) \ge F_{\mu(X)}^{-1}(p)).
        \]
    \end{corollary}

    \begin{proposition}
        Let $Y = \mu(X + \delta) + \epsilon$, where $X$, $\delta$, and $\epsilon$ are independent random
        variables. If $\mu$ is increasing, and $F_X$ is continuous at $F_X^{-1}(p)$, then an optimal predictor of $\I(Y > F_Y^{-1}(p))$ is $\I(X > F_X^{-1}(p))$.
    \end{proposition}
\end{frame}

\begin{frame}{Performance Metrics}
    Let $\I(g(X) \in S)$ be a calibrated predictor of $\I(Y > F_Y^{-1}(p))$. We call
    \[
    \P(g(X) \in S \mid Y > F_Y^{-1}(p)), \ \P(g(X) \in S \mid Y \le F_Y^{-1}(p))
    \]
    the true positive rate (TPR) and false positive rate (FPR), respectively, of the predictor.

    \medskip
    
    Let $A = \{g(X) \in S\}$ and $E = \{Y > F_Y^{-1}(p)\}$. We can express the TPR and FPR in terms of the precision $\lambda_p$:
    \[
    \TPR = \P(A \mid E) = \frac{\P(A)\P(E \mid A)}{\P(E)} = \frac{(1 - p)\lambda_p}{1 - p} = \lambda_p.
    \]
    \[
    \FPR = \P(A \mid E^c) = \frac{\P(A)\P(E^c \mid A)}{\P(E^c)} = \frac{(1 - p)(1 - \lambda_p)}{p}.
    \]
\end{frame}

\begin{frame}{Performance Metrics}
    Many metrics are used to evaluate predictor performance. Two examples are the TSS and the F1 score. Again letting $\lambda_p$ be the precision, we have
    \[
    \text{TSS} := \text{TPR} - \text{FPR} = \lambda_p - \frac{(1 - p)(1 - \lambda_p)}{p} = \frac{1}{p}\lambda_p - \frac{1 - p}{p}
    \]
    and
    \[
    \text{F1} := \frac{2\lambda_p \cdot \text{TPR}}{\lambda_p + \text{TPR}} = \frac{2\lambda_p^2}{2\lambda_p} = \lambda_p
    \]
    so maximizing the precision is equivalent to maximizing the TSS and the F1 score.
\end{frame}

\section{Optimal Prediction for AR($d$) Models}

\begin{frame}{The Pareto Distribution}
    For a random variable $X$ with CDF $F_X$, let $\bar{F}_X = 1 - F_X$.

    \smallskip
    
    $X$ has a Pareto distribution with shape parameter $\alpha > 0$ and scale parameter $x_m > 0$ if
    \[
    \bar{F}_X(x) = \left(\frac{x}{x_m}\right)^{-\alpha}
    \]
    for all $x \ge x_m$. Note that given $\lambda > 0$, for $x \ge \lambda x_m$,
    \[
    \P(\lambda X > x) = \bar{F}_X(x / \lambda) = \left(\frac{x}{\lambda x_m}\right)^{-\alpha}.
    \]
    The Pareto distribution is scale invariant.
\end{frame}

\begin{frame}{The Pareto Distribution}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.3]{scaled_paretos.png}
        \caption{A Pareto($\alpha = 2, x_m = 1$) distribution (left) and under rescalings (center, right) (from \cite{nair2022thef}).}
        \label{fig:scaled_paretos}
    \end{figure}
\end{frame}

\begin{frame}{Scale Invariance}
    \begin{definition}
        (The distribution of) $X$ is scale invariant if there exists $x_0 > 0$ and a continuous positive function $g$ such that
        \[
        \bar{F}_X(\lambda x) = g(\lambda)\bar{F}_X(x),
        \]
        for all $x, \lambda$ satisfying $x, \lambda x \ge x_0$.
    \end{definition}
    \begin{theorem}
        (The distribution of) $X$ is scale invariant if and only if $F_X$ has a power law tail, that is, there exists $x_0 > 0$, $c \ge 0$, and $\alpha > 0$ such that $\bar{F}_X(x) = c x^{-\alpha}$ for $x \ge x_0$.
    \end{theorem}
\end{frame}

\begin{frame}{Regularly and Slowly Varying Functions}
    For any function $g$ defined for all large $x$, if there exists $\alpha \in \R$ such that for all large $x$,
    \[
    \lim_{t \to \infty} \frac{g(t x)}{g(t)} = x^{-\alpha},
    \]
    then $g$ is called regularly varying with index $\alpha$ (written $g \in \RV_{\alpha}$). If $\alpha = 0$, then $g$ is called slowly varying.
    \begin{theorem}
        $g$ \in $\RV_{\alpha}$ if and only if there exists $\ell \in \RV_0$ such that for all large $x$,
        \[
        g(x) = x^{\alpha}\ell(x).
        \]
    \end{theorem}
\end{frame}

\begin{frame}{Asymptotic Scale Invariance}
    \begin{definition}
        (The distribution of) $X$ is asymptotically scale invariant if there exists a continuous positive function $g$ such that for any $\lambda > 0$,
        \[
        \lim_{x \to \infty} \frac{\bar{F}_X(\lambda x)}{\bar{F}_X(x)} = g(\lambda).
        \]
    \end{definition}
    \begin{theorem}
        (The distribution of) $X$ is asymptotically scale invariant if and only if it is regularly varying.
    \end{theorem}
\end{frame}

\begin{frame}{Regularly Varying Random Variables}
    \begin{definition}
        Let $X$ be a random variable defined on a probability space $(\Omega, F, P)$. $X$ is regularly varying with tail index $\alpha > 0$ if
        \begin{itemize}
            \item for all large $x$,
            \[
            \lim_{t \to \infty} \frac{\bar{F}_{|X|}(t x)}{\bar{F}_{|X|}(t)} = x^{-\alpha}.
            \]
            \item (tail balance condition) there exists $p_X \in [0, 1]$ such that
            \[
            \lim_{x \to \infty} \frac{\bar{F}_X(x)}{\bar{F}_{|X|}(x)} = p_X.
            \]
            $p_X$ is called the extremal skewness of (the distribution of) $X$.
        \end{itemize}
    \end{definition}
\end{frame}

\begin{frame}{Regularly Varying Random Variables}
    A simpler version we have been using:
    \begin{definition}
        A random variable $X$ is regularly varying (or heavy-tailed) with tail exponent $\alpha > 0$ if there exist $\sigma_{X, \pm} \ge 0$ such that
         % Unstar if you want to use the label
        \begin{equation*}\label{eq:tail_conds}
            P(X > x) \sim \sigma_{X, +}^{\alpha}x^{-\alpha} \text{ and } P(X < -x) \sim \sigma_{X, -}^{\alpha}x^{-\alpha}
        \end{equation*}
        as $x \to \infty$, where $\sigma_{X, +} + \sigma_{X, -} > 0$. The numbers $\sigma_{X, \pm}$ are called the right and left asymptotic scale coefficients, respectively.
    \end{definition}
\end{frame}

% \begin{frame}{Preliminaries on Regular Variation}
%     \begin{lemma}\label{lem:reg_var_properties}
%         Let $X$ and $Y$ be independent random variables that are regularly varying with tail exponent $\alpha$ and asymptotic scale coefficients $\sigma_{X, \pm}$ and $\sigma_{Y, \pm}$, respectively. Then
    
%         (i) For any $a \ne 0$, $a X$ is regularly varying with tail exponent $\alpha$ and asymptotic scale coefficients $|a|\sigma_{X, \pm{\rm sign}(a)}$.
        
%         (ii) $Z = X + Y$ is regularly varying with tail exponent $\alpha$ and asymptotic scale coefficients $\sigma_{Z, \pm}$, where $\sigma_{Z, \pm}^{\alpha} = \sigma_{X, \pm}^{\alpha} + \sigma_{Y, \pm}^{\alpha}$.
%     \end{lemma}
% \end{frame}

\begin{frame}{The Tail Dependence Coefficient}
    Recall that the level-$p_0$ tail dependence coefficient $\lambda_{p_0} = \lambda_{p_0}(Y, X)$ is defined by
    \[
    \lambda_{p_0} = P(Y > F_Y^{-1}(p_0) \mid X > F_X^{-1}(p_0))
    \]
    and the tail dependence coefficient $\lambda = \lambda(Y, X)$ is defined by
    \[
    \lambda = \lim_{p_0 \uparrow 1} \lambda_{p_0}
    \]
    if the limit exists.
\end{frame}

\begin{frame}{The Tail Dependence Coefficient}
    \begin{lemma}\label{lem:lambda_lem}
        Let $X$ and $\epsilon$ be independent random variables that are regularly varying with tail exponent $\alpha$ and asymptotic scale coefficients $\sigma_{X, \pm}$ and $\sigma_{\epsilon, \pm}$, respectively. For any $a \ne 0$, if $\sigma_{X, \sign(a)} > 0$, then
        \[
        \lambda(a X + \epsilon, X) = \frac{|a|^\alpha \sigma_{X, \sign(a)}^\alpha}{|a|^\alpha \sigma_{X, \sign(a)}^\alpha + \sigma_{\epsilon, +}^\alpha}.
        \]
    \end{lemma}
\end{frame}

\begin{frame}{The AR($d$) Model}
    Consider the autoregressive (AR) model of order $d$
    \begin{equation}\label{eq:AR_d_mod}
        Y_t = \sum_{j = 1}^d \phi_j Y_{t - j} + \epsilon_t, \ t \in \Z.
    \end{equation}
    Assume that
    \begin{itemize}
        \item the $\epsilon_t$'s are iid
        \item for all $t$, $\epsilon_t$ is independent of $Y_{t - 1}, Y_{t - 2}, \ldots$
    \end{itemize}

    \medskip

    \textbf{
        Goal:
        \begin{center}
            Predict whether $Y_{t + h} > F_Y^{-1}(p)$ using $Y_s$ for $s \le t$
        \end{center}
    }
\end{frame}

\begin{frame}{Stationarity of the AR($d$) Model}
    Define $\pi : \C \to \C$ by
    \[
    \pi(z) = 1 - \sum_{j = 1}^d \phi_j z^j
    \]
    If $\pi$'s roots lie outside the closed unit disk, then the AR($d$) model \eqref{eq:AR_d_mod} has a unique stationary solution that is causal:
    % Under certain conditions, this series may converge absoluteley almost surely. What conditions are those?
    \begin{equation}\label{eq:AR_d_mod_solution}
        Y_t = \sum_{j = 0}^\infty a_j \epsilon_{t - j},
    \end{equation}
    where the $a_j$'s are given by
    \[
    \sum_{j = 0}^\infty a_j z^j = 1 / \pi(z)
    \]
    when $|z| \le 1$.
\end{frame}

\begin{frame}{Two Useful Sequences}
    Let $\{Y_t\}$ be an AR($d$) stochastic process satisfying \eqref{eq:AR_d_mod}.

    \smallskip
    
    Define $\{\phi(h)\}_{h = -(d - 1)}^{\infty} \subset \R^d$ and $\{\psi(h)\}_{h = -(d - 1)}^{\infty} \subset \R^h$ as follows.
    
    When $-(d - 1) \le h \le 0$, set
    \[
    \phi_j(h) = \delta_{|h|, j}, \quad 0 \le j \le d - 1 \label{eq:phi_recur_rel1}, \quad \text{and} \quad
    \psi_j(h) = 0, \quad 1 \le j \le h
    \]
    $\delta$ being the Kronecker delta. For $h \ge 1$, define $\phi(h)$ and $\psi(h)$ by
    \begin{align*}
        \phi_j(h) &= \sum_{i = 1}^d \phi_i\phi_j(h - i), \quad 0 \le j \le d - 1 \\
        \psi_j(h) &=
        \begin{cases}
            \sum_{i = 1}^{(h - j) \wedge d} \phi_i\psi_j(h - i), & 1 \le j \le h - 1 \\
            1, & j = h
        \end{cases}
    \end{align*}
\end{frame}

\begin{frame}{Two Useful Sequences}
    Some observations:
    \begin{itemize}
        \item Let $\{e_1, \ldots, e_d\}$ be the standard basis of $\R^d$. Then
        \[
        \phi(0) = e_1, \ldots, \phi(-(d - 1)) = e_d.
        \]
        \item For $h \ge 1$, $\phi(h) = (\begin{matrix} \phi(h - 1) & \cdots & \phi(h - d) \end{matrix})\phi$.
        \item $\phi(1) = (\begin{matrix} \phi(0) & \cdots & \phi(-(d - 1)) \end{matrix})\phi = (\begin{matrix} e_1 & \cdots & e_d \end{matrix})\phi = \phi$.
    \end{itemize}
    \begin{lemma}
        (i) For all $h \ge -(d - 1)$,
        \[
        Y_{t + h} = \sum_{j = 0}^{d - 1} \phi_j(h)Y_{t - j} + \sum_{j = 1}^h \psi_j(h)\epsilon_{t + j}.
        \]
        (ii) Let $\Phi = (\begin{matrix} \phi & e_1 & \cdots & e_{d - 1} \end{matrix}) \in \R^{d \times d}$. Then $\phi(h) = \Phi^h e_1$ for all $h \ge 1$.
    \end{lemma}
\end{frame}

\begin{frame}{The Optimal Predictor for AR($d$) Models}
    \begin{theorem}
        Let $\{Y_t\}$ be the unique stationary solution of the AR$(d)$ equation in \eqref{eq:AR_d_mod}.
        The optimal predictor of $\I(Y_{t + h} > F_Y^{-1}(p))$ via
        \[
        Y_{t:(t - d + 1)} := (Y_t, \ldots, Y_{t - d + 1})^{\top}
        \]
        is $\I(\AROptPred{t}{h}{\phi} > F_{\AROptPred{t}{h}{\phi}}^{-1}(p))$, where
        \[
        \AROptPred{t}{h}{\phi} := \phi(h)^{\top}Y_{t:(t - d + 1)} = \sum_{j = 0}^{d - 1} \phi_j(h)Y_{t - j}.
        \]
    \end{theorem}
\end{frame}

\begin{frame}{Approximating the Optimal Predictor}
    \begin{itemize}
        \item Given an estimator $\hat{\phi}$ of $\phi$, the matrix $\Phi$ can be approximated by the matrix $\hat{\Phi} = (\begin{matrix} \hat{\phi} & e_1 & \cdots & e_{d - 1} \end{matrix})$.
        \item This yields plug-in approximations:
        \[
        \hat{\phi}(h) = \hat{\Phi}^h e_1 \text{ for } \phi(h), \quad \approxAROptPred{t}{h}{\phi} = \hat{\phi}(h)Y_{t:(t - d + 1)} \text{ for } \AROptPred{t}{h}{\phi}.
        \]
        \item $\phi$ can be estimated using methods like LAD and OLS estimation from a training set consisting of $n$ pairs
        \[
        (Y_t, Y_{(t - 1):(t - d)}), \ldots, (Y_{t - n + 1}, Y_{(t - n):(t - n - d + 1)}).
        \]
        \item Under certain conditions, if $\hat{\phi}$ is a LAD or OLS estimator, $\hat{\phi} \xrightarrow{\P} \phi$. See, e.g., \cite{davis1992mest}.
    \end{itemize}
\end{frame}

\begin{frame}{Consistency of the Approximation}
    \begin{theorem}
        Let $\phi$ be the coefficient vector in the AR($d$) model \eqref{eq:AR_d_mod}, and let $\hat{\phi}(n) := \hat{\phi}$ be an estimator of $\phi$, with $n$ being the size of the training set. Suppose that $\hat{\phi} \xrightarrow{\P} \phi$ as $n \to \infty$. Then $\hat{\phi}(h) \xrightarrow{\P} \phi(h)$ and $\approxAROptPred{t}{h}{\phi} \xrightarrow{\P} \AROptPred{t}{h}{\phi}$.
    \end{theorem}
    In practice, we would use a predictor of the form
    \[
    I(\approxAROptPred{t}{h}{\phi} > \hat{q}_n),
    \]
    where $\hat{q}_n$ is an estimate of $F_{\approxAROptPred{t}{h}{\phi}}^{-1}(p)$, like the $p$th sample quantile of
    \[
    \{\approxAROptPred{t}{h}{\phi}, \ldots, \approxAROptPred{t - n + 1}{h}{\phi}\}
    \]
\end{frame}

\begin{frame}{Consistency of the Approximation}
    \begin{theorem}
        Suppose that $\hat{\phi} \xrightarrow{\P} \phi$ and $\hat{q}_n \xrightarrow{\P} q := F_{\AROptPred{t}{h}{\phi}}^{-1}(p)$.
        
        Also suppose that $F_Y^{-1}(p)$ is a continuity point of $F_Y$ and that $q$ is a continuity point of $F_{\AROptPred{t}{h}{\phi}}$.
        
        Then $\I(\approxAROptPred{t}{h}{\phi} > \hat{q}_n)$ is:
        
        (i) Asymptotically calibrated, i.e., as $n \to \infty$,
        \[
        \P(\approxAROptPred{t}{h}{\phi} > \hat{q}_n) \to 1 - p = \P(Y_{t + h} > F_Y^{-1}(p))
        \]
        
        (ii) Asymptotically optimal, i.e., as $n \to \infty$,
        \[
        \P(Y_{t+h} > F_Y^{-1}(p) \mid \approxAROptPred{t}{h}{\phi} > \hat{q}_n) \to \lambda_p(Y_{t + h}, \AROptPred{t}{h}{\phi})
        \]
    \end{theorem}
\end{frame}

\begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
    Consider the linear state-space model
    \begin{equation}\label{eq:orig_ssm}
        \begin{split}
            X_t &= \sum_{j = 1}^q A_j X_{t - j} + \delta_t \\
            Y_t &= g \left( \sum_{j = 0}^{r - 1} \beta_j^{\top} X_{t - j} \right) + \epsilon_t,
        \end{split}
    \end{equation}
    where
    \begin{itemize}
        \item $X_t \in \R^d$ and the $\delta_t$'s are iid heavy-tailed
        \item $Y_t \in \R$, $g :\R \to \R$ is increasing, and the $\epsilon_t$'s are iid heavy--tailed with tail exponent $\alpha > 0$
        \item the $\delta_t$'s and the $\epsilon_t$'s are independent
    \end{itemize}
\end{frame}

\begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
    Goal:
    \begin{center}
        Predict extreme events of the form $\{Y_{t + h} > F_Y^{-1}(p_0)\}$ using $\{X_s, \ s \le t\}$    
    \end{center}

    \medskip
    
    We may assume that $q = r$. Also, there exist $A \in \R^{dq \times dq}$ and $\beta \in \R^{dq}$ such that a solution to
    \begin{equation}\label{eq:new_ssm}
    \tilde X_t := A \tilde X_{t-1} + \tilde \delta_t\ \ \mbox{ and }\ \ Y_t = g(\beta^{\top}\tilde X_t) + \epsilon_t,
    \end{equation}
    where
    \[
    \tilde{X}_t := (X_t^{\top} \cdots X_{t - q + 1}^{\top})^{\top} \ \ \text{and} \ \
    \tilde{\delta}_t := (\begin{matrix} \delta_t^{\top} & 0 & \cdots & 0 \end{matrix})^{\top},
    \]
    is a solution to the original model \eqref{eq:orig_ssm}, so we may assume that $q = r = 1$.
\end{frame}

% \begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
%     Set
%     \[
%     A :=
%     \left(\begin{array}{lllll}
%         A_1 & A_2 & \cdots & A_{q-1} & A_q\\
%         I_d & 0 & \cdots & 0 & 0 \\
%         0  & I_d & \cdots & 0& 0\\
%         \vdots & \vdots &  \ddots &\vdots & \vdots\\
%         0 & 0 & \cdots & I_d & 0
%     \end{array}\right) \ \ \mbox{ and } \ \
%     \beta := \left(\begin{array}{l}
%         \beta_1 \\ \beta_2 \\ \vdots \\ \beta_q
%     \end{array}\right) 
%     \]
% \end{frame}

\begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
    Relation \eqref{eq:new_ssm} implies
    \begin{align*}
        Y_{t+h} &= g \Big( \beta^\top ( A \tilde X_{t+h-1} + \tilde  \delta_{t+h})\Big ) + \epsilon_{t+h} \\
        &= g \Big( \beta^\top A \tilde X_{t + h-1} +  \beta^\top \tilde  \delta_{t+h}  \Big) + \epsilon_{t+h} \\
        &\vdots \\
        &= g \Big( \beta^\top A^h \tilde X_t + \sum_{j=1}^h \beta^\top A^{h - j}\tilde  \delta_{t+j}  \Big) +  \epsilon_{t+h} \\
        &= g \Big( \beta^\top A^h \tilde X_t + \delta_{t,h} \Big) + \epsilon_{t+h},
    \end{align*}
    where 
    \begin{equation*}
    \delta_{t,h}:= \sum_{j=1}^h \beta^\top A^{h - j}\tilde  \delta_{t+j}.
    \end{equation*}
\end{frame}

\begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
    \begin{proposition}
        Let $\{(\tilde X_t,Y_t)\}$ be a solution to the model \eqref{eq:new_ssm}. Assume that
        \begin{itemize}
            \item $\tilde{\delta}_s, \ s \ge t + 1$ and $\epsilon_s, \ s \ge t + 1$ are independent of $\tilde{X}_s, \ s \le t$
            \item $\tilde X_t$, $\delta_{t, h}$, and $\epsilon_{t+h}$ have densities
        \end{itemize}

        \medskip
        
        {\em (i)} The optimal predictor of $I(Y_{t+h}> F_{Y_{t+h}}^{-1}(p_0))$
        via $\{ \tilde X_s, s\le t\}$ takes the form $I(\beta^\top A^{h}\tilde X_t > \tau)$.
    \end{proposition}
\end{frame}

\begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
    \begin{proposition}
        {\em (ii)} Suppose that the $\tilde{\delta}_t$'s are multivariate regularly varying with exponent $\nu>0$ and that the $\epsilon_t$'s are regularly varying with exponent
        $\alpha>0$ and scale coefficient $\sigma_{\epsilon,+}>0$.  Suppose moreover that $\{ (\tilde X_t, Y_t)\}$ is stationary, and
        that the function $g$ in the model \eqref{eq:new_ssm} is the identity. \\
        
        Then, the asymptotically optimal lag-$h$ precision is
        \begin{equation*}
        \lambda(Y_{t+h}, \beta^\top A^h \tilde X_t) = \left\{ \begin{array}{ll} 
        0 &, \mbox{ if } \nu>\alpha\\
        1- \sigma_{\delta_{t,h},+}^\nu / \sigma_{\beta^\top \tilde X_{0},+}^\nu &, \mbox{ if } \nu < \alpha\\
        1- (\sigma_{\delta_{t,h},+}^\nu + \sigma_{\epsilon,+}^\alpha) / \sigma_{Y,+}^\alpha &,\ \mbox{ if } \nu = \alpha,
        \end{array}\right.
        \end{equation*}
    \end{proposition}
\end{frame}    

% \begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
%     \begin{remark}\label{rem:ss_phase_transition}
%         Observe that in the case when the measurement errors $\epsilon_k$ have strictly heavier tails 
%         than the innovations $\delta_k$ in the state-space equation, i.e., $\nu>\alpha$, it follows that the 
%         asymptotically optimal prediction precision is zero for all lags $h\ge 1$.  Indeed, in this  case, the measurement
%         noise simply dominates the state-space signal and there is vanishing information in the covariates $\{X_k\}$ about the extremes of $Y_k$.
%     \end{remark}
% \end{frame}

% \begin{frame}{State-Space Models for Heavy-Tailed Time Series with Covariates}
%     \begin{remark}\label{rem:ss_phase_transition_gauss}
%         If $\{(\tilde X_k, Y_k)\}$ is Gaussian and ${\rm Var}(\epsilon_{k+h})>0$ then the asymptotically
%         optimal precision  $\lambda(Y_{k+h}, \beta^\top A^h \tilde X_k) = 0$, for all $h\ge 1$. This follows, as in 
%         Remark \ref{rem:Gaussian-vanishing-precision}, from the fact that the jointly Gaussian random variables 
%         $Y_{k+h}$ and $\beta^\top A^h \tilde X_k$ have asymptotically independent extremes.
%     \end{remark}
% \end{frame}

\begin{frame}{A Simulation Study}
    Data was generated from the state-space model
    \begin{align*}
        X_t &= \sum_{j = 1}^5 0.5^j I_{16}X_{t - j} + \delta_t \\
        Y_t &= \sum_{j = 0}^4 0.5^j 1_{16}^{\top}X_{t - j} + \epsilon_t
    \end{align*}
    where
    \begin{itemize}
        \item $X_t \in \R^{16}$, $\delta_t \sim t_{\nu_{\delta}}(0, I_{16})$
        \item $Y_t \in \R$, $\epsilon_t \sim t_{\nu_{\epsilon}}$
        \item the $\delta_t$'s and $\epsilon_t$'s are mutually independent
    \end{itemize}
\end{frame}

\begin{frame}{A Simulation Study}
    Let 
    \[
    A =
    \left(
    \begin{matrix}
        0.5 \cdot I_{16} & 0.5^2 I_{16} & 0.5^3 I_{16} & 0.5^4 I_{16} & 0.5^5 I_{16} \\
        I_{16} & 0 & 0 & 0 & 0 \\
        0 & I_{16} & 0 & 0 & 0 \\
        0 & 0 & I_{16} & 0 & 0 \\
        0 & 0 & 0 & I_{16} & 0
    \end{matrix}
    \right), \
    \beta =
    \left(
    \begin{matrix}
        1_{16} \\
        0.5 \cdot 1_{16} \\
        0.5^2 1_{16} \\
        0.5^3 1_{16} \\
        0.5^4 1_{16}
    \end{matrix}
    \right)
    \]
    \[
    \tilde{X}_t = 
    \left(
    \begin{matrix}
        X_t^{\top} & X_{t - 1}^{\top} & X_{t - 2}^{\top} & X_{t - 3}^{\top} & X_{t - 4}^{\top}
    \end{matrix}
    \right)^{\top}
    \]
    The optimal predictor of $I(Y_{t + 1} > F_{Y_{t + 1}}^{-1}(p))$ is
    \[
    I(\beta^{\top} A\tilde{X}_t > \tau)
    \]
    for some $\tau$.
\end{frame}

\begin{frame}{A Simulation Study}
    Training and test sets were of the form
    \[
    \left(
    \begin{matrix}
    X_1^{\top} & \cdots & X_{-3}^{\top} & Y_2 \\
    \vdots & \ddots & \vdots & \vdots \\
    X_n^{\top} & \cdots & X_{n - 4}^{\top} & Y_{n + 1} \\
    \end{matrix}
    \right)
    =
    \left(
    \begin{matrix}
    \tilde{X}_1^{\top} & Y_2 \\
    \vdots & \vdots \\
    \tilde{X}_n^{\top} & Y_{n + 1} \\
    \end{matrix}
    \right).
    \]

    Given a new observation $\tilde{X}_*$, predict that $Y_*$ is extreme if $\beta^{\top} A\tilde{X}_*$ is above the $p$th sample quantile of $\{\beta^{\top} A\tilde{X}_1, \ldots, \beta^{\top} A\tilde{X}_n\}$.
\end{frame}

\begin{frame}{A Simulation Study}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{sim_study07.png}
        \caption{Results of the simulation study.}
        \label{fig:sim_study07}
    \end{figure}
\end{frame}

\section{Data for Solar Flare Forecasting}

\begin{frame}{Data Information}
    \begin{itemize}
        \item Response (X-ray flux) data comes from the GOES satellites.
        \item Covariate (SHARP parameter) data comes from the Solar Dynamics Observatory (SDO).
    \end{itemize}

    The response and covariate data was combined as follows:
    \begin{itemize}
        \item Impute missing flux and SHARP parameter values
        \item Aggregate SHARP parameter data across HARPs 
        \item Join aggregated SHARP time series to flux time series
    \end{itemize}
    The final time series consists of $\num[group-separator={,}]{287635}$ observations.
\end{frame}

\begin{frame}{Missing and Low Quality Values}
    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{flux_20170906.png}
        \caption{The X-Ray flux, 9/3/17-9/9/17.}
        \label{fig:flux_20170906}
    \end{figure}
\end{frame}

\begin{frame}{Missing and Low Quality Values}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.5]{harp23.png}
        \caption{SHARP parameter time series for HARP 23.}
        \label{fig:na_props_flux}
    \end{figure}
\end{frame}

\begin{frame}{Missing and Low Quality Values}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.5]{harp2995.png}
        \caption{SHARP parameter time series for HARP 2995.}
        \label{fig:na_props_flux}
    \end{figure}
\end{frame}

\begin{frame}{High Dimensionality}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.5]{intrinsic_dim_ests.png}
        \caption{Estimates of the intrinsic dimension of the SHARP parameters.}
        \label{fig:intrinsic_dim_ests}
    \end{figure}
\end{frame}

\begin{frame}{Heavy Tails}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{hill_ests.png}
        \caption{Hill estimates of $\xi$ computed over consecutive $\num[group-separator={,}]{10000}$-observation windows of the flux time series. The windows included in this plot were randomly selected.}
        \label{fig:hill_ests}
    \end{figure}
\end{frame}

\begin{frame}{Heavy Tails}
    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{hill_plot.png}
        \caption{One particular Hill plot for the X-Ray flux.}
        \label{fig:hill_plot}
    \end{figure}
\end{frame}

\section{A Statistical Framework for Solar Flare Forecasting}
\label{sect:forecast_framework}

\begin{frame}{Preprocessing Approaches}
    Marginal transformations:
    \begin{itemize}
        \item Standardization
        \begin{itemize}
            \item $X_{i j} \rightarrow (X_{i j} - \bar{X}_j) / s_j$
        \end{itemize}
        \item Let $\hat{F}_j$ be the empirical CDF of SHARP parameter $j$.
        \item Rank transformation
        \begin{itemize}
            \item $X_{i j} \rightarrow \hat{F}_j(X_{i j})$
        \end{itemize}
        \item Pareto transformation
        \begin{itemize}
            \item $X_{i j} \rightarrow \frac{1}{1 - \hat{F}_j(X_{i j})}$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Preprocessing Approaches}
    Nonlinear dimensionality reduction:
    \begin{itemize}
        \item One technique is locally linear embedding (LLE) \cite{roweis2000nonl}.
        \item LLE assumes that observations $X_1, \ldots, X_n \in \R^D$ lie on/near a manifold of dimension $d \ll D$.
        \item LLE steps:
        \begin{enumerate}
            \item Find the $K$ NNs of each $X_i$.
            \item For each $X_i$, calculate weights $w_{i j}$ for reconstructing $X_i$ from its neighbors as $\sum_j w_{i j}X_j$.
            \item Compute $Y_1, \ldots, Y_n \in \R^d$ to minimize $\sum_{i = 1}^n \|Y_i - \sum_j w_{i j}Y_j\|_2^2$.
        \end{enumerate}
        \item $Y_1, \ldots, Y_n$ are the embeddings of $X_1, \ldots, X_n$, respectively, in the manifold.
        \item We apply LLE to construct four composite features from the SHARP parameters.
    \end{itemize}
\end{frame}

\begin{frame}{Preprocessing Approaches}
    Spline transformation of covariates:
    \begin{itemize}
        \item A linear combination of the covariates has the form
        \[
        X^{\top}\beta = \sum_{i = 1}^d \beta_i X_i.
        \]
        \item To increase flexibility, we replace $\beta_i X_i$ with $s_i(X_i)$, where $s_i$ is a natural spline.
        \item Sample quantiles of $X_i$ are used as knots.
    \end{itemize}
\end{frame}

\begin{frame}{Forecasting via Empirical Tail Dependence Maximization}
    Given training data $(X_1, Y_1), \ldots, (X_n, Y_n)$, the level-$p_0$ empirical tail dependence coefficient is
    \[
    \hat{\lambda}_{p_0}(Y, g(X)) = \frac{1}{n(1 - p_0)}\sum_{i = 1}^n I(g(X_i) \ge \hat{F}_{g(X)}^{-1}(p_0))I(Y_i \ge \hat{F}_{Y}^{-1}(p_0)),
    \]
    We compute
    \[
    \hat{g} = \argmin_{g \in \mathcal{C}} \left[-\hat{\lambda}_{p_0}(Y, g(X)) + \text{penalty}(g)\right].
    \]
\end{frame}

\begin{frame}{Forecasting via Empirical Tail Dependence Maximization}
    A natural class of predictors:
    \[
    \mathcal{C} = \{g : g(x) = x^{\top}\beta, \beta \in S^{d - 1}\}.
    \]
    We compute
    \[
    \hat{\beta} = \argmin_{\beta \in S^{d - 1}} \left[-\hat{\lambda}_{p_0}(Y, X^{\top}\beta) + \lambda\|\beta - \hat{\beta}_{\text{prev}}\|_2\right].
    \]
    Set $\hat{g}(x) = x^{\top}\hat{\beta}$. Prediction steps:
    \begin{itemize}
        \item Calculate the $p$th sample quantile $\hat{F}_{\hat{g}(X)}^{-1}(p_0)$ of $\hat{g}(X_1), \ldots, \hat{g}(X_n)$.
        \item Given a new observation $X_*$, predict that $Y_* > F_{Y}^{-1}(p_0)$ if $\hat{g}(X_*) > \hat{F}_{\hat{g}(X)}^{-1}(p_0)$.
    \end{itemize}
\end{frame}

\begin{frame}{Results}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.5]{0601_results.png}
        \caption{Results for LLE-constructed covariates.}
        \label{fig:0601_results}
    \end{figure}
\end{frame}

\begin{frame}{Results}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.5]{0101_results.png}
        \caption{More results, for hand-picked covariates.}
        \label{fig:0101_results}
    \end{figure}
\end{frame}

\begin{frame}{Results}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.25]{0901_results.png}
        \caption{More results, for hand-picked covariates and imputed data.}
        \label{fig:0901_results}
    \end{figure}
\end{frame}

\begin{frame}{Results}
    \begin{figure}[!htb]
        \centering
        \includegraphics[scale=0.25]{0901_results.png}
        \caption{More results, for hand-picked covariates and imputed data.}
        \label{fig:0901_results}
    \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \printbibliography
\end{frame}

\section{Appendix}

\begin{frame}{HEK Entry, 9/6/17 X9.3 Flare}
    \begin{figure}
        \centering
        \includegraphics[scale=0.3]{hek_entry_20170906.png}
        \caption{Heliophysics Events Knowledgebase (HEK) entry, 9/6/17 X9.3 flare.}
        \label{fig:hek_entry}
    \end{figure}
\end{frame}

\begin{frame}{Soft X-Ray Flux Values, 9/6/17 X9.3 Flare}
    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{flux_vals_20170906.png}
        \caption{Soft X-Ray Flux Values, 9/6/17 X9.3 Flare.}
        \label{fig:flux_vals_20170906}
    \end{figure}
\end{frame}

\end{document}
\documentclass{beamer}

\mode<presentation> {
\usetheme{AnnArbor}
}

\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage[shortlabels]{enumitem}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

\newcommand{\MDA}{\text{MDA}}
\newtheorem{proposition}{Proposition}

\title[Statistical Methods for Extremal Events]{Statistical Methods for Extremal Events}

\author{Victor Verma}
\institute[]
{
Prof. Yang Chen's Reading Group \\
Department of Statistics \\
University of Michigan
}
\date[2/23/23]{2/23/23}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Today's Reading}
    \begin{itemize}
        \item Chapter 6 of \textit{Modelling Extremal Events} by Embrechts, Kl\"{u}ppelberg, and Mikosch (\cite{embrechts_et_al_1997})
    \end{itemize}
\end{frame}

%\begin{frame}{Outline}
%    \tableofcontents
%\end{frame}

\begin{frame}{Parameter Estimation for the GEV Distribution}
    Recall that a GEV (generalized extreme value) distribution $H_{\xi; \mu, \psi}$ satisfies
    \begin{align*}
        H_{\xi; \mu, \psi}(x) &= \exp\left\{-\left(1 + \xi\frac{x - \mu}{\psi}\right)^{-1 / \xi}\right\}, \quad 1 + \xi(x - \mu) / \psi > 0 \\
        &=
        \begin{cases}
            \Phi_{\alpha}\left(1 + \frac{x - \mu}{\alpha\psi}\right), & x > \mu - \psi\alpha, \ \xi = 1 / \alpha > 0 \\
            \Psi_{\alpha}\left(-\left(1 - \frac{x - \mu}{\alpha\psi}\right)\right), & x < \mu + \psi\alpha, \ \xi = -1 / \alpha < 0 \\
            \Lambda\left(\frac{x - \mu}{\psi}\right), & x \in \mathbb{R}, \ \xi = 0
        \end{cases}
    \end{align*}
    where
    \begin{align*}
        \text{(Fr\'{e}chet)} \quad \Phi_{\alpha}(x) &= \exp\left\{-x^{-\alpha}\right\}I_{(0, \infty)}(x) \\
        \text{(Weibull)} \quad \Psi_{\alpha}(x) &= \exp\{-(-x)^{\alpha}\}I_{(-\infty, 0)}(x) + I_{[0, \infty)}(x) \\
        \text{(Gumbel)} \quad \Lambda(x) &= \exp\left\{-e^{-x}\right\}
    \end{align*}
\end{frame}

\begin{frame}{Parameter Estimation for the GEV Distribution}
    \begin{itemize}
        \item $\xi \in \mathbb{R}$ is a shape parameter
        \item $\mu \in \mathbb{R}$ is a location parameter
        \item $\psi \in \mathbb{R}_+$ is a scale parameter
    \end{itemize}
    We write $H_{\xi; \mu, \psi}$ as $H_{\theta}$, where $\theta = (\xi, \mu, \psi)$. We also write $H_{\xi; 0, 1}$ as $H_{\xi}$.

    \smallskip

    Question: if $X_1, \ldots, X_n \overset{\text{iid}}{\sim} H_{\theta}$, how can we estimate $\theta$?
\end{frame}

\begin{frame}{Parameter Estimation for the GEV Distribution}
    $\theta$ can be estimated using maximum likelihood estimation.

    \medskip
    
    If $\xi \ne 0$, the log-likelihood is
    \begin{align*}
        \ell(\theta; \mathbf{X}) &= 
        -n\log\sigma \\
        &\quad - (1 + 1 / \xi)\sum_{i = 1}^n \log\left[1 + \xi\left(\frac{X_i - \mu}{\sigma}\right)\right] \\
        &\quad - \sum_{i = 1}^n \left[1 + \xi\left(\frac{X_i - \mu}{\sigma}\right)\right]^{-1 / \xi}
    \end{align*}
    when $1 + \xi(X_i - \mu) / \sigma > 0$ for $i = 1, \ldots, k$; it is $-\infty$ otherwise. If $\xi = 0$, the log-likelihood is
    \[
    \ell(\mu, \psi; \mathbf{X}) = -n\log\psi - \sum_{i = 1}^n \left(\frac{X_i - \mu}{\psi}\right) - \sum_{i = 1}^n \exp\left[-\left(\frac{X_i - \mu}{\psi}\right)\right]
    \]
\end{frame}

\begin{frame}{Parameter Estimation for the GEV Distribution}
    Numerical methods are required to compute the MLE in both cases.

    \medskip

    The MLE is efficient, consistent, and asymptotically normal in regular cases. A non-regular case occurs when the support of $H_{\theta}$ depends on $\theta$. In non-regular cases, the MLE may not have nice properties even if it can be estimated.

    \medskip

    It is known that the MLE has nice properties when $\xi > -1 / 2$ and that it doesn't when $\xi \le -1 / 2$. If the support of $H_{\theta}$ is unbounded above, then $\xi \ge 0 > -1 / 2$.
\end{frame}

\begin{frame}{Estimating Under MDA Conditions}
    \begin{definition}
        We say that the rv $X$ (the df $F$ of $X$, the distribution of $X$) belongs to the \textbf{maximum domain of attraction} of the extreme value distribution $H_{\xi; \mu, \psi}$ if there exist constants $c_n > 0, d_n \in \mathbb{R}$ such that $c_n^{-1}(M_n - d_n) \xrightarrow{d} H_{\xi; \mu, \psi}$. We write $X \in \MDA(H_{\xi; \mu, \psi})$ or $F \in \MDA(H_{\xi; \mu, \psi})$. $c_n$ and $d_n$ are called \textbf{norming constants}.
    \end{definition}
    Some facts about MDAs:
    \begin{itemize}
        \item $F \in \MDA(H_{\xi; \mu, \psi})$ with norming constants $c_n > 0, d_n \in \mathbb{R}$ iff $\lim_{n \to \infty} n\bar{F}(c_n x + d_n) = -\log H_{\xi; \mu, \psi}(x)$ for all $x \in \mathbb{R}$.
        \item $\MDA(H_{\xi; \mu, \psi}) = \MDA(H_{\xi; 0, 1}) = \MDA(H_{\xi})$.
        \item $F \in \MDA(\Phi_{\alpha})$ iff $\bar{F} \in \mathcal{R}_{-\alpha}$, i.e., $\bar{F}$ is \textbf{regularly varying}; $\lim_{t \to \infty} \bar{F}(t x) / \bar{F}(t) = x^{-\alpha}$ for all $x > 0$.
    \end{itemize}
\end{frame}

\begin{frame}{Estimating Under MDA Conditions}
    Question: if $X_1, \ldots, X_n \overset{\text{iid}}{\sim} F \in \MDA(H_{\xi})$, how can we estimate $\xi$?

    \medskip

    An example showing how this differs from the previous question:

    \medskip
    
    Let $\Phi_{\alpha}(x) = \exp\{-x^{-\alpha}\}, x > 0$ is the df of the standard $\alpha$-Fr\'{e}chet distribution. \\
    If $X_1, \ldots, X_n \overset{iid}{\sim} F = \Phi_{\alpha}$, then
    \[
    \bar{F}(x) = 1 - \exp\{-x^{-\alpha}\}, x > 0.
    \]
    If $X_1, \ldots, X_n \overset{iid}{\sim} F \in \MDA(\Phi_{\alpha})$, then $\bar{F} \in \mathcal{R}_{-\alpha}$, so
    \[
    \bar{F}(x) = x^{-\alpha}L(x), x > 0,
    \]
    where $L$ is \textbf{slowly varying}, i.e., $L \in \mathcal{R}_0$.
\end{frame}

\begin{frame}{Estimating Under MDA Conditions}
    Since $X_1, \ldots, X_n \overset{\text{iid}}{\sim} F \in \MDA(H_{\xi})$, for $u = c_n x + d_n$,
    \begin{align*}
        n\bar{F}(u) &\approx -\log H_{\xi}(x) \\
        &\approx \left(1 + \xi\frac{u - d_n}{c_n}\right)^{-1 / \xi} \\
        \bar{F}(u) &\approx \frac{1}{n}\left(1 + \xi\frac{u - d_n}{c_n}\right)^{-1 / \xi}.
    \end{align*}
    With estimates $\hat{\xi}$, $\hat{c_n}$, and $\hat{d_n}$, we could estimate $\bar{F}(u)$ as
    \[
    \widehat{\bar{F}(u)} = \frac{1}{n}\left(1 + \hat{\xi}\frac{u - \hat{d_n}}{\hat{c_n}}\right)^{-1 / \hat{\xi}}.
    \]
\end{frame}

\begin{frame}{Estimating Under MDA Conditions}
    $F \in \MDA(H_{\xi})$ is a tail property (e.g., $F \in \MDA(\Phi_{\alpha}) \implies \bar{F} \in \mathcal{R}_{-\alpha}$), so $\hat{\xi}$ can be computed from the $k$ largest order statistics $X_{k, n} \le \ldots \le X_{1, n}$.

    \medskip

    Typical conditions imposed on $k = k(n)$:
    \begin{itemize}
        \item $\lim_{n \to \infty} k(n) = \infty$
        \item $\lim_{n \to \infty} \frac{n}{k(n)} = \infty$
    \end{itemize}
\end{frame}

\begin{frame}{Pickands's Estimator for $\xi \in \mathbb{R}$}
    The \textbf{Pickands estimator} of $\xi$ is defined as
    \[
    \hat{\xi}_{k, n}^{(P)} = \frac{1}{\log 2}\log\frac{X_{k, n} - X_{2 k, n}}{X_{2 k, n} - X_{4 k, n}}
    \]
\end{frame}

\begin{frame}{Pickands's Estimator for $\xi \in \mathbb{R}$}
    \begin{proposition}
        Suppose $\{X_n\}$ is an iid sequence with df $F \in \MDA(H_{\xi})$, $\xi \in \mathbb{R}$. Let $\hat{\xi}^{(P)} = \hat{\xi}_{k, n}^{(P)}$ be the Pickands estimator.
        \begin{enumerate}
            \item[(a)] (Weak consistency) If $k \to \infty$, $k / n \to 0$ for $n \to \infty$, then $\hat{\xi}^{(P)} \xrightarrow{P} \xi, \quad n \to \infty$.
            \item[(b)] (Strong consistency) If $k / n \to 0$, $k / \log\log n \to \infty$ for $n \to \infty$, then 
            $\hat{\xi}^{(P)} \xrightarrow{a.s.} \xi, \quad n \to \infty$.
            \item[(c)] (Asymptotic normality) Under further conditions on $k$ and $F$,
            \[
            \sqrt{k}(\hat{\xi}^{(P)} - \xi) \xrightarrow{d} N(0, v(\xi)), \quad n \to \infty,
            \]
            where
            \[
            v(\xi) = \frac{\xi^2(2^{2\xi + 1} + 1)}{(2(2^{\xi} - 1)\log 2)^2}.
            \]
        \end{enumerate}
    \end{proposition}    
\end{frame}

\begin{frame}{Pickands Plots}
    A \textbf{Pickands plot} is a graph of the form
    \[
    \left\{\left(k, \hat{\xi}_{k, n}^{(P)}\right) : k = 1, \ldots, n\right\}.
    \]
    We choose a $k$ that belongs to a run of $k$-values where the graph is roughly flat. We then estimate $\xi$ as $\hat{\xi}_{k, n}^{(P)}$.
\end{frame}

\begin{frame}{Pickands Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{pickands_plot.png}
        \caption{An example Pickands plot (source: \cite{embrechts_et_al_1997}).}
        \label{fig:pickands_plot}
    \end{figure}
\end{frame}

\begin{frame}{A Pickands Horror Plot}
    \begin{figure}
        \centering
        \includegraphics[scale=0.32]{pickands_horror_plots.png}
        \caption{More Pickands plots; the one on the right is a horror plot (source: \cite{nair_et_al_2022}).}
        \label{fig:pickands_horror_plots}
    \end{figure}
\end{frame}

\begin{frame}{Hill's Estimator for $\xi = \alpha^{-1} > 0$}
    Assume that $X_1, \ldots, X_n \overset{\text{iid}}{\sim} F \in \MDA(\Phi_{\alpha})$, $\alpha > 0$.

    \medskip

    The \textbf{Hill estimator} of $\alpha$ is
    \[
    \hat{\alpha}^{(H)} = \hat{\alpha}_{k, n}^{(H)} = \left(\frac{1}{k}\sum_{j = 1}^k \log X_{j, n} - \log X_{k, n}\right)^{-1}.
    \]
\end{frame}

\begin{frame}{Hill's Estimator for $\xi = \alpha^{-1} > 0$}
    \begin{proposition}
        Suppose $\{X_n\}$ is an iid sequence with df $F \in \MDA(\Phi_{\alpha})$, $\alpha > 0$. Let $\hat{\alpha}^{(H)} = \hat{\alpha}_{k, n}^{(H)}$ be the Hill estimator.
        \begin{enumerate}
            \item[(a)] (Weak consistency) If $k \to \infty$, $k / n \to 0$ for $n \to \infty$, then $\hat{\alpha}^{(H)} \xrightarrow{P} \alpha, \quad n \to \infty$.
            \item[(b)] (Strong consistency) If $k / n \to 0$, $k / \log\log n \to \infty$ for $n \to \infty$, then 
            $\hat{\alpha}^{(H)} \xrightarrow{a.s.} \alpha, \quad n \to \infty$.
            \item[(c)] (Asymptotic normality) Under further conditions on $k$ and $F$,
            \[
            \sqrt{k}(\hat{\alpha}^{(H)} - \alpha) \xrightarrow{d} N(0, \alpha^2)
            \]
        \end{enumerate}
    \end{proposition}    
\end{frame}

\begin{frame}{Hill Plots}
    A \textbf{Hill plot} is a graph of the form
    \[
    \left\{\left(k, \hat{\alpha}_{k, n}^{(H)}\right) : k = 2, \ldots, n\right\}.
    \]
    We choose a $k$ that belongs to a run of $k$-values where the graph is roughly flat. We then estimate $\alpha$ as $\hat{\alpha}_{k, n}^{(H)}$.
\end{frame}

\begin{frame}{Hill Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{hill_plot.png}
        \caption{An example Hill plot (source: \cite{embrechts_et_al_1997}).}
        \label{fig:hill_plot}
    \end{figure}
\end{frame}

\begin{frame}{Hill Horror Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.3]{hill_horror_plots1.png}
        \caption{Hill horror plots (source: \cite{nair_et_al_2022}).}
        \label{fig:hill_horror_plots1}
    \end{figure}
\end{frame}

\begin{frame}{Hill Horror Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.3]{hill_horror_plots2.png}
        \caption{More Hill horror plots (source: \cite{nair_et_al_2022}).}
        \label{fig:hill_horror_plots2}
    \end{figure}
\end{frame}

\begin{frame}{The Dekkers-Einmahl-de Haan Estimator for $\xi \in \mathbb{R}$}
    The Hill estimator is only for $F \in \MDA(H_{\xi})$, where $\xi > 0$. The \textbf{Dekkers-Einmahl-de Haan estimator} extends it to all $\xi \in \mathbb{R}$. It is defined as
    \[
    \hat{\xi} = H_n^{(1)} + 1 + \frac{1}{2}\left(\frac{(H_n^{(1)})^2}{H_n^{(2)}} - 1\right)^{-1},
    \]
    where
    \begin{align*}
        H_n^{(1)} &= \frac{1}{k}\sum_{j = 1}^k (\log X_{j, n} - \log X_{k + 1, n}) \\
        H_n^{(2)} &= \frac{1}{k}\sum_{j = 1}^k (\log X_{j, n} - \log X_{k + 1, n})^2.
    \end{align*}
    This estimator is also called the \textbf{moments estimator}. 
\end{frame}

\begin{frame}{The Dekkers-Einmahl-de Haan Estimator for $\xi \in \mathbb{R}$}
    \begin{proposition}
        Suppose $\{X_n\}$ is an iid sequence with df $F \in \MDA(H_{\xi})$, $\xi \in \mathbb{R}$, such that $x_F > 0$. Let $\hat{\xi}^{(M)} = \hat{\xi}_{k, n}^{(M)}$ be the moments estimator.
        \begin{enumerate}
            \item[(a)] (Weak consistency) If $k \to \infty$, $k / n \to 0$ for $n \to \infty$, then $\hat{\xi}^{(M)} \xrightarrow{P} \xi, \quad n \to \infty$.
            \item[(b)] (Strong consistency) If $k \to \infty$, $k / n \to 0$, $k /(\log n)^{\delta} \to \infty$ for some $\delta > 0$ for $n \to \infty$, then 
            $\hat{\xi}^{(M)} \xrightarrow{a.s.} \xi, \quad n \to \infty$.
        \end{enumerate}
    \end{proposition}    
\end{frame}

\begin{frame}{Moments Plots}
    A \textbf{moments plot} is a graph of the form
    \[
    \left\{\left(k, \hat{\xi}_{k, n}^{(M)}\right) : k = 1, \ldots, n\right\}.
    \]
    We choose a $k$ that belongs to a run of $k$-values where the graph is roughly flat. We then estimate $\xi$ as $\hat{\xi}_{k, n}^{(M)}$.
\end{frame}

\begin{frame}{Moments Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{moments_plot.png}
        \caption{An example moments plot (source: \cite{embrechts_et_al_1997}).}
        \label{fig:moments_plot}
    \end{figure}
\end{frame}

\begin{frame}{A Moments Horror Plot}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{moments_horror_plots.png}
        \caption{More moments plots; the one on the right is a horror plot (source: \cite{nair_et_al_2022}).}
        \label{fig:moments_horror_plots}
    \end{figure}
\end{frame}

\begin{frame}{Threshold Excesses}
    Let $X, X_1, X_2, \ldots$ be iid rvs with common df $F$. Two ways of thinking about extremes:
    \begin{itemize}
        \item The maximum $\vee_{i = 1}^n X_i$
        \item The \textbf{threshold excess} $X - u$ when $X > u$, where $u$ is a high threshold
    \end{itemize}
    The block maximum approach uses maxima. It can waste useful data if some blocks contain multiple extreme values.

    \begin{definition}
        Let $X$ be a rv with df $F$ and right endpoint $x_F$. For a fixed $u < x_F$,
        \[
        F_u(x) = P(X - u \le x | X > u), \ x \ge 0,
        \]
        is the \textbf{excess df} of the rv $X$ (of the df $F$) over the threshold $u$.
    \end{definition}
\end{frame}

\begin{frame}{Threshold Excesses}
    \begin{proposition} % Theorem 4.1 in Coles
        Let $X, X_1, X_2, \ldots$ be iid rvs with common df $F$ and let $M_n = \max_{i = 1, \ldots, n} X_i$. Suppose that $F$ is in the MDA of a GEV distribution; for large $n$, $P(M_n \le x) \approx H_{\theta}(x)$, where
        \[
        H_{\theta}(x) = \exp\left\{-\left[1 + \xi\left(\frac{x - \mu}{\psi}\right)\right]^{-1 / \xi}\right\}
        \]
        for some $\mu$, positive $\psi$, and $\xi$. Then for large enough $u$,
        \[
        F_u(x) \approx 1 - \left(1 + \frac{\xi x}{\tilde{\psi}}\right)^{-1 / \xi}
        \]
        on $\{x : x > 0, 1 + \xi x / \tilde{\psi} > 0\}$, where $\tilde{\psi} = \psi + \xi(u - \mu)$.
    \end{proposition}
\end{frame}

\begin{frame}{The Generalized Pareto Distribution}
    A distribution with df of the form
    \[
    1 - \left(1 + \frac{\xi x}{\psi}\right)^{-1 / \xi}
    \]
    supported on $\{x : x > 0, 1 + \xi x / \psi > 0\}$, with $\xi \in \mathbb{R}$ and $\psi > 0$, is called a \textbf{generalized Pareto (GP) distribution}. When $\xi = 0$, this is defined by continuity as $1 - e^{-x / \psi}$, the df of the exponential distribution with mean $\psi$.
\end{frame}

\begin{frame}{The Mean Excess Function}
    \begin{definition}
        Let $X$ be a rv with right endpoint $x_F$; then
        \[
        e(u) = E(X - u | X > u), 0 \le u < x_F,
        \]
        is called the \textbf{mean excess function} of $X$.
    \end{definition}
\end{frame}

\begin{frame}{The Mean Excess Function}
    Suppose that $X$ is a positive rv with df $F$ and finite expectation. Then
    \begin{align*}
        e(u) &= \frac{1}{\bar{F}(u)}\int_u^{x_F} (x - u)\,dF(x) \\
        &= \frac{1}{\bar{F}(u)}\int_u^{x_F} \bar{F}(x)\,dx, \quad 0 < u < x_F.
    \end{align*}
    If $F$ is continuous,
    \[
    \bar{F}(x) = \frac{e(0)}{e(x)}\exp\left\{-\int_0^x \frac{1}{e(u)}\,du\right\}.
    \]
    This implies that a continuous df is uniquely determined by its mean excess function.
\end{frame}

\begin{frame}{The Mean Excess Function}
    \begin{figure}
        \centering
        \includegraphics[scale=0.32]{mean_excess_functions.png}
        \caption{Some mean excess functions (source: \cite{embrechts_et_al_1997})}
        \label{fig:mean_excess_functions}
    \end{figure}
\end{frame}

\begin{frame}{The Mean Excess Function}
    \begin{proposition}
        Suppose $X$ has GPD with parameters $\xi < 1$ and $\sigma$. Then for $u < x_F$,
        \[
        e(u) = E(X - u | X > u) = \frac{\sigma + \xi u}{1 - \xi}, \ \sigma + \xi u > 0.
        \]
    \end{proposition}
\end{frame}

\begin{frame}{The Empirical Mean Excess Function}
    \begin{definition}
        Let $X_1, \ldots, X_n \overset{\text{iid}}{\sim} F$, let $F_n$ be the empirical df, and let $\triangle_n(u) = \{i : i = 1, \ldots, n, \ X_i > u\}$. The \textbf{empirical mean excess function} $e_n$ is defined by
        \[
        e_n(u) = \frac{1}{\#\triangle_n(u)}\sum_{i \in \delta_n(u)} (X_i - u), \ u \ge 0,
        \]
        with the convention that $0 / 0 = 0$.
    \end{definition}
\end{frame}

\begin{frame}{An Example Empirical Mean Excess Function}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{exponential_mean_excess_function.png}
        \caption{The empirical mean excess function for standard exponential data (source: \cite{embrechts_et_al_1997}).}
        \label{fig:exponential_mean_excess_function}
    \end{figure}
\end{frame}

\begin{frame}{An Example Empirical Mean Excess Function}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{lognormal_mean_excess_function.png}
        \caption{The empirical mean excess function for lognormal ($\log X \sim N(0, 4)$) data (source: \cite{embrechts_et_al_1997}).}
        \label{fig:lognormal_mean_excess_function}
    \end{figure}
\end{frame}

\begin{frame}{An Example Empirical Mean Excess Function}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{pareto_mean_excess_function.png}
        \caption{The empirical mean excess function for Pareto (tail index = 1.7) data (source: \cite{embrechts_et_al_1997}).}
        \label{fig:pareto_mean_excess_function}
    \end{figure}
\end{frame}

\begin{frame}{Mean Residual Life Plots}
    Suppose that $F_u$ can be modeled as a GPD for large $u$. How can these $u$ be identified?

    \smallskip

    One approach is to make a \textbf{mean residual life plot}. This is a plot of points of the form $(u, e_n(u))$. Let $\xi$ and $\sigma$ be the parameters of the approximating GPD. Then for large $u$,
    \[
    e_n(u) \approx e(u) \approx \frac{\sigma + \xi u}{1 - \xi}
    \]
    Suitable $u$ are those for which $(u, e_n(u))$ roughly fall along a line.

    \smallskip

    Mean residual life plots can be difficult to interpret, as the example on the next slide shows.
\end{frame}

\begin{frame}{Mean Residual Life Plots}
    \begin{figure}
        \centering
        \includegraphics[scale=0.35]{mean_residual_life_plot.png}
        \caption{An example mean residual life plot (source: \cite{coles_2001}).}
        \label{fig:mean_residual_life_plot}
    \end{figure}
\end{frame}

\begin{frame}{GPD Parameter Estimation}
    A GPD is fit using maximum likelihood estimation. Suppose that there are $k$ excesses $Y_1, \ldots, Y_k$ of the selected threshold $u$. If $\xi \ne 0$, then the log-likelihood is
    \[
    \ell(\xi, \psi) = -k\log\psi - (1 + 1 / \xi)\sum_{i = 1}^k \log(1 + \xi Y_i / \psi)
    \]
    when $1 + \xi Y_i / \psi > 0$ for $i = 1, \ldots, k$; otherwise, $\ell(\xi, \psi) = -\infty$. If $\xi = 0$, the log-likelihood is
    \[
    \ell(\psi) = -k\log\psi - \frac{1}{\psi}\sum_{i = 1}^k Y_i.
    \]
    Numerical methods are required to find the MLEs.
\end{frame}

\begin{frame}{References}
    \nocite{*}
    \printbibliography
\end{frame}

\end{document}